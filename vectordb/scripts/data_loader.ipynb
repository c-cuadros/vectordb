{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import os\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from typing_extensions import Unpack\n",
    "from abc import ABC\n",
    "from typing import List, TypedDict, overload\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from db.vector_db import Item, VectorDB\n",
    "from db.qdrant import Qdrant\n",
    "os.environ[\"UNIVERSE_DIR\"] = (\"/mnt/disk1/ecad_database/features/65k/universe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader from CIA-EV-IDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from identificationWorker.utils import file_utils\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import deepdish as dd\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_unique_by_order(data):\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    w = list(OrderedDict.fromkeys(data))\n",
    "    return w\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    A class for loading and managing data for the identification engine.\n",
    "\n",
    "    Args:\n",
    "        universe_dir (str): The directory path where the universe data is stored.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, universe_dir):\n",
    "        self.universe_mapping = []\n",
    "        self.universe_features = []\n",
    "        self.universe_context_idx = []\n",
    "        self.ready = False\n",
    "        self.universe_dir = universe_dir\n",
    "        self.timestamp_dataloader = None\n",
    "        self.universe_selection = []\n",
    "\n",
    "    def get_status(self) -> bool:\n",
    "        return self.ready\n",
    "\n",
    "    def set_status(self, st: bool):\n",
    "        self.ready = st\n",
    "\n",
    "    def get_features_path(self, selection: list[str] = None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the paths of the .h5 feature files.\n",
    "\n",
    "        Args:\n",
    "            selection (list[str], optional): A list of directory names to filter the search. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of paths to the .h5 feature files.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no .h5 files are found in the specified directory/directories.\n",
    "        \"\"\"\n",
    "        h5_list = []\n",
    "        if selection is not None:\n",
    "            for dir_name in selection:\n",
    "                h5_files = glob.glob(os.path.join(self.universe_dir, dir_name, \"*.h5\"))\n",
    "\n",
    "                if not h5_files:\n",
    "                    raise ValueError(f\"No .h5 files found in directory {dir_name}\")\n",
    "                h5_list.extend(h5_files)\n",
    "        else:\n",
    "            h5_list = glob.glob(os.path.join(self.universe_dir, \"**/*.h5\"))\n",
    "            if not h5_list:\n",
    "                raise ValueError(\n",
    "                    f\"No .h5 files found in the universe directory {self.universe_dir}\"\n",
    "                )\n",
    "\n",
    "        return h5_list\n",
    "\n",
    "    def load_h5_features(self, file: str) -> dict:\n",
    "        try:\n",
    "            # dd -> python2, 3 compatibility for h5 files\n",
    "            file_data = dd.io.load(file)\n",
    "\n",
    "            if \"segments\" in file_data:\n",
    "                return file_data[\"segments\"][0]\n",
    "            else:\n",
    "                return file_data\n",
    "        except Exception:\n",
    "            print(f\"Error: {file} not found or corrupt data.\")\n",
    "            return None\n",
    "\n",
    "    def get_h5data_dict(self, work_dict):\n",
    "    \n",
    "        file = os.path.join(self.universe_dir, work_dict[\"obra\"], work_dict[\"fonograma\"] + \".h5\")\n",
    "\n",
    "        features = self.load_h5_features(file)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def get_h5data(self, work_id, track_id):    \n",
    "        file = os.path.join(self.universe_dir, work_id, track_id + \".h5\")\n",
    "\n",
    "        features = self.load_h5_features(file)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def batch_load_h5data(self, list_files):\n",
    "        pass\n",
    "\n",
    "    def load_universe(self, selection: list[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Loads the universe data by iterating over the provided selection of files.\n",
    "\n",
    "        Args:\n",
    "            selection (list[str], optional): A list of file paths to load. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the universe data is successfully loaded, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no universe files data is found.\n",
    "        \"\"\"\n",
    "        universe_features_path: list[str] = self.get_features_path(selection)\n",
    "\n",
    "        if not universe_features_path:\n",
    "            raise ValueError(\"No universe files data found.\")\n",
    "        else:\n",
    "            for file in universe_features_path:\n",
    "                parts = file.split(\"/\")\n",
    "                work_id = parts[-2]\n",
    "                track_id = parts[-1].split(\".\")[0]\n",
    "                self.universe_mapping.append({\"obra\": work_id, \"fonograma\": track_id})\n",
    "                self.universe_selection.append(f\"{work_id}\")\n",
    "        universe_features_path = []\n",
    "\n",
    "        return True\n",
    "\n",
    "    def set_universe(self, selection: list[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Prepares the data loader by loading the universe data and setting the readiness status.\n",
    "\n",
    "        Args:\n",
    "            selection (list[str], optional): List of universe selections. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the universe features and mapping are loaded successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        self.ready = False\n",
    "        self.universe_mapping = []\n",
    "        self.universe_features = []\n",
    "        self.universe_context_idx = []\n",
    "        self.universe_selection = []\n",
    "        if self.load_universe(selection):\n",
    "            if len(self.universe_mapping) > 0:\n",
    "                self.ready = True\n",
    "                # self.universe_selection = selection\n",
    "\n",
    "    def get_universe(self):\n",
    "        return self.universe_mapping\n",
    "\n",
    "    def get_universe_idx(self):\n",
    "        return range(len(self.universe_mapping))\n",
    "\n",
    "    def get_universe_mapping(self):\n",
    "        return self.universe_mapping\n",
    "\n",
    "    def _search_restricted_universe(self, universe_context=None):\n",
    "        self.universe_context_idx = []\n",
    "\n",
    "        for idx, u in enumerate(self.universe_mapping):\n",
    "            if u[\"obra\"] in universe_context:\n",
    "                self.universe_context_idx.append(idx)\n",
    "\n",
    "        return self.universe_context_idx\n",
    "\n",
    "    def get_restricted_universe(\n",
    "        self, universe_context_by_id=None, universe_context_by_idx=None\n",
    "    ):\n",
    "        if universe_context_by_id is None and universe_context_by_idx is None:\n",
    "            return (\n",
    "                self.get_universe_idx(),\n",
    "                self.universe_mapping,\n",
    "            )\n",
    "\n",
    "        self.universe_context_idx = []\n",
    "        # universe_restricted = []\n",
    "        universe_restricted_mapping = []\n",
    "\n",
    "        if universe_context_by_id is not None:\n",
    "            for idx, u in enumerate(self.universe_mapping):\n",
    "                if u[\"obra\"] in universe_context_by_id:\n",
    "                    self.universe_context_idx.append(idx)\n",
    "                    # universe_restricted.append(self.universe_features[idx])\n",
    "                    universe_restricted_mapping.append(u)\n",
    "\n",
    "        else:\n",
    "            for idx in universe_context_by_idx:\n",
    "                self.universe_context_idx.append(idx)\n",
    "                # universe_restricted.append(self.universe_features[idx])\n",
    "                universe_restricted_mapping.append(self.universe_mapping[idx])\n",
    "\n",
    "        return (\n",
    "            self.universe_context_idx,\n",
    "            # universe_restricted,\n",
    "            universe_restricted_mapping,\n",
    "        )\n",
    "\n",
    "    def get_restricted_universe_idx(self, universe_context=None):\n",
    "        if universe_context is not None:\n",
    "            self._search_restricted_universe(universe_context=universe_context)\n",
    "\n",
    "        if len(self.universe_context_idx) == 0:\n",
    "            return self.get_universe_idx()\n",
    "\n",
    "        return self.universe_context_idx\n",
    "\n",
    "    def get_work_from_idx(self, list_idx):\n",
    "        worksList = []\n",
    "        for idx in list_idx:\n",
    "            worksList.append(self.universe_mapping[idx][\"obra\"])\n",
    "\n",
    "        return get_unique_by_order(worksList)\n",
    "\n",
    "    def get_fonograma_from_idx(self, list_idx):\n",
    "        fonogramaList = []\n",
    "        for idx in list_idx:\n",
    "            fonogramaList.append(self.universe_mapping[idx][\"fonograma\"])\n",
    "\n",
    "        return fonogramaList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIVERSE_DIR = os.getenv(\"UNIVERSE_DIR\")\n",
    "dl = DataLoader(UNIVERSE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.load_universe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filter(TypedDict):\n",
    "    obra: str\n",
    "    fonograma: str\n",
    "\n",
    "\n",
    "class  FeatureRepository(ABC):\n",
    "    def __init__(self, vector_db: VectorDB):\n",
    "        self.vector_db = vector_db\n",
    "        self.collection_name = None\n",
    "        self.dim = None\n",
    "\n",
    "    def try_create_collection(self):\n",
    "        self.vector_db.try_create_collection(self.collection_name, self.dim)\n",
    "    \n",
    "    # TODO: Add basic logic to process the feature\n",
    "    def process(self, feature: np.ndarray):\n",
    "        return feature\n",
    "\n",
    "    def add(self, features: list[np.ndarray], payloads=None):\n",
    "        for i, feature in enumerate(features):\n",
    "            vectors = [vector for vector in feature]\n",
    "            payload = payloads[i] if payloads else None\n",
    "            if payload:\n",
    "                payload = [payload for _ in vectors]\n",
    "            \n",
    "            self.vector_db.add(self.collection_name, vectors, payload)\n",
    "\n",
    "    def search(self, queries: np.ndarray, top_k=5, filter: dict = None):\n",
    "        for feature in queries:\n",
    "            vectors = [vector for vector in feature]\n",
    "            yield self.vector_db.search(\n",
    "                self.collection_name, vectors, top_k=top_k, filter=filter\n",
    "            )\n",
    "    \n",
    "    @overload\n",
    "    def get(self, id: str) -> List[Item]: ...\n",
    "\n",
    "    @overload\n",
    "    def get(self, filter: Filter, top_k=5) -> List[Item]: ...\n",
    "        \n",
    "    def get(self, id_or_filter: str = None, top_k=5):\n",
    "        return self.vector_db.get(self.collection_name, id_or_filter, top_k)\n",
    "\n",
    "    def delete(self, id: str):\n",
    "        self.vector_db.delete(self.collection_name, id)\n",
    "\n",
    "    def delete_collection(self):\n",
    "        self.vector_db.delete_collection(self.collection_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQTNetRepository(FeatureRepository):\n",
    "    def __init__(self, vector_db: VectorDB):\n",
    "        self.vector_db = vector_db\n",
    "        self.collection_name = \"cqtnet_test\" # TODO: Mudar para o nome correto\n",
    "        self.dim = 300\n",
    "\n",
    "class CQTNetV1Repository(FeatureRepository):\n",
    "    def __init__(self, vector_db: VectorDB):\n",
    "        self.vector_db = vector_db\n",
    "        self.collection_name = \"cqtnet_v1_test\" # TODO: Mudar para o nome correto\n",
    "        self.dim = 300\n",
    "\n",
    "class FusionRepository(FeatureRepository):\n",
    "    def __init__(self, vector_db: VectorDB):\n",
    "        self.vector_db = vector_db\n",
    "        self.collection_name = \"fusion_test\" # TODO: Mudar para o nome correto\n",
    "        self.dim = 300\n",
    "\n",
    "class CoverhunterRepository(FeatureRepository):\n",
    "    def __init__(self, vector_db: VectorDB):\n",
    "        self.vector_db = vector_db\n",
    "        self.collection_name = \"coverhunter_test\" # TODO: Mudar para o nome correto\n",
    "        self.dim = 128\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqtnet_repository = CQTNetRepository(qdrant)\n",
    "coverhunter_repository = CoverhunterRepository(qdrant)\n",
    "cqtnetv1_repository = CQTNetV1Repository(qdrant)\n",
    "fusion_repository = FusionRepository(qdrant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialize collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Index for collection 'cqtnet_test' already exists.\n",
      "WARNING:root:Index for collection 'coverhunter_test' already exists.\n",
      "WARNING:root:Index for collection 'cqtnet_v1_test' already exists.\n",
      "WARNING:root:Index for collection 'fusion_test' already exists.\n"
     ]
    }
   ],
   "source": [
    "cqtnet_repository.try_create_collection()\n",
    "coverhunter_repository.try_create_collection()\n",
    "cqtnetv1_repository.try_create_collection()\n",
    "fusion_repository.try_create_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeaturesDbLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessOptions(TypedDict):\n",
    "    granularity: str\n",
    "    with_confidency: bool\n",
    "\n",
    "class FeaturesDbLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_loader: DataLoader,\n",
    "        cqtnet_repository: CQTNetRepository,\n",
    "        cqtnetv1_repository: CQTNetV1Repository,\n",
    "        coverhunter_repository: CoverhunterRepository,\n",
    "        fusion_repository: FusionRepository,\n",
    "    ):\n",
    "        self.data_loader = data_loader\n",
    "        self.cqtnet_repository = cqtnet_repository\n",
    "        self.cqtnetv1_repository = cqtnetv1_repository\n",
    "        self.coverhunter_repository = coverhunter_repository\n",
    "        self.fusion_repository = fusion_repository\n",
    "\n",
    "    def get_feature_vector(\n",
    "        self, feature: dict, feature_name: str, **pre_process_options: Unpack[PreProcessOptions]\n",
    "    ) -> np.ndarray:\n",
    "        if pre_process_options:\n",
    "            return self.pre_process_features(\n",
    "                feature,\n",
    "                feature_name,\n",
    "                pre_process_options.get(\"granularity\", \"all\"),\n",
    "                pre_process_options.get(\"with_confidency\", True),\n",
    "            )\n",
    "        return feature[feature_name][\"features\"]\n",
    "\n",
    "    def load_features(self, *features: str, **pre_process_options: Unpack[PreProcessOptions]):\n",
    "        features_path = self.data_loader.get_features_path()        \n",
    "        with Pool(processes=4) as pool:\n",
    "            features_list = pool.map(self.data_loader.load_h5_features, features_path)\n",
    "            # feature_args = [(feature_name, features_list, pre_process_options) for feature_name in features]\n",
    "            # results = pool.map(self._process_feature_name, feature_args)\n",
    "        # features_list = [\n",
    "        #     self.data_loader.load_h5_features(path) for path in features_path\n",
    "        # ]\n",
    "\n",
    "        results = []\n",
    "        for feature_name in features:\n",
    "            if feature_name == \"fusion\":\n",
    "                vectors_cqtnet = [\n",
    "                    self.get_feature_vector(feature, \"cqtnet\", **pre_process_options)\n",
    "                    for feature in features_list\n",
    "                ]\n",
    "                vectors_cqtnet_v1 = [\n",
    "                    self.get_feature_vector(feature, \"cqtnet_v1\", **pre_process_options)\n",
    "                    for feature in features_list\n",
    "                ]\n",
    "                vectors = [\n",
    "                    (v1 + v2) / 2 for v1, v2 in zip(vectors_cqtnet, vectors_cqtnet_v1)\n",
    "                ]\n",
    "            elif feature_name in [\"cqtnet\", \"coverhunter\", \"cqtnet_v1\"]:\n",
    "                vectors = [\n",
    "                    self.get_feature_vector(feature, feature_name, **pre_process_options)\n",
    "                    for feature in features_list\n",
    "                ]\n",
    "            else:\n",
    "                return None\n",
    "            results.append(vectors)\n",
    "\n",
    "        results.append(dl.get_universe_mapping())\n",
    "\n",
    "        return tuple(results)\n",
    "\n",
    "    def add_features(self, *features: str, **pre_process_options: Unpack[PreProcessOptions]):\n",
    "        features_payload = self.load_features(*features, **pre_process_options)\n",
    "        vectors = features_payload[: len(features_payload) - 1]\n",
    "        payloads = features_payload[-1]\n",
    "        feature_repository_map: dict[str, FeatureRepository] = {\n",
    "            \"cqtnet\": cqtnet_repository,\n",
    "            \"cqtnet_v1\": cqtnetv1_repository,\n",
    "            \"coverhunter\": coverhunter_repository,\n",
    "            \"fusion\": fusion_repository,\n",
    "        }\n",
    "\n",
    "        for f, v in zip(features, vectors):\n",
    "            feature_repository_map[f].add(v, payloads)\n",
    "\n",
    "    def pre_process_features(\n",
    "        self, feature: dict, feature_name: str, granularity: str, with_confidence=True\n",
    "    ):\n",
    "        chroma_feat = feature_name\n",
    "\n",
    "        if with_confidence:\n",
    "            if \"features\" in feature[\"confidence\"]:\n",
    "                if granularity == \"all\":\n",
    "                    vectors = feature[chroma_feat][\"features\"]\n",
    "                    confidence = self._ret_confidence(feature[\"confidence\"][\"features\"])\n",
    "                elif granularity == \"only_all_song\":\n",
    "                    vectors = feature[chroma_feat][\"features\"][None, -1, :]\n",
    "                    confidence = 1.0\n",
    "                else:\n",
    "                    vectors = feature[chroma_feat][\"features\"][0:-1, :]\n",
    "                    confidence = self._ret_confidence(\n",
    "                        feature[\"confidence\"][\"features\"][0:-1, :]\n",
    "                    )\n",
    "            elif \"confidence\" in feature:\n",
    "                if granularity == \"all\":\n",
    "                    vectors = feature[chroma_feat]\n",
    "                    confidence = self._ret_confidence(feature[\"confidence\"])\n",
    "                elif granularity == \"only_all_song\":\n",
    "                    vectors = feature[chroma_feat][None, -1, :]\n",
    "                    confidence = 1.0\n",
    "                else:\n",
    "                    vectors = feature[chroma_feat][0:-1, :]\n",
    "                    confidence = self._ret_confidence(feature[\"confidence\"][0:-1, :])\n",
    "            else:\n",
    "                if granularity == \"all\":\n",
    "                    vectors = feature[chroma_feat]\n",
    "                elif granularity == \"only_all_song\":\n",
    "                    vectors = feature[chroma_feat][None, -1, :]\n",
    "                else:\n",
    "                    vectors = feature[chroma_feat][0:-1, :]\n",
    "\n",
    "                confidence = 1.0\n",
    "\n",
    "        else:\n",
    "            if \"features\" in feature[chroma_feat]:\n",
    "                if feature[chroma_feat][\"features\"].shape[0] > 1:\n",
    "                    if granularity == \"all\":\n",
    "                        vectors = feature[chroma_feat][\"features\"]\n",
    "                    elif granularity == \"only_all_song\":\n",
    "                        vectors = feature[chroma_feat][\"features\"][None, -1, :]\n",
    "                    else:\n",
    "                        vectors = feature[chroma_feat][\"features\"][0:-1, :]\n",
    "                else:\n",
    "                    vectors = feature[chroma_feat][\"features\"]\n",
    "            else:\n",
    "                if feature[chroma_feat].shape[0] > 1:\n",
    "                    if granularity == \"all\":\n",
    "                        vectors = feature[chroma_feat]\n",
    "                    elif granularity == \"only_all_song\":\n",
    "                        vectors = feature[chroma_feat][None, -1, :]\n",
    "                    else:\n",
    "                        vectors = feature[chroma_feat][0:-1, :]\n",
    "                else:\n",
    "                    vectors = feature[chroma_feat]\n",
    "\n",
    "        vectors = normalize(vectors, norm=\"l2\")\n",
    "\n",
    "        if with_confidence:\n",
    "            size = min(vectors.shape[0], len(confidence))\n",
    "            vectors = (vectors[:size, :] * confidence[:size, :]).astype(np.float32)\n",
    "            # vectors = (vectors * confidence).astype(\"float32\")\n",
    "        else:\n",
    "            vectors = vectors.astype(\"float32\")\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def _process_feature_name(self, feature_name, features_list, **pre_process_options):\n",
    "        if feature_name == \"fusion\":\n",
    "            vectors_cqtnet = [\n",
    "                self.get_feature_vector(feature, \"cqtnet\", **pre_process_options)\n",
    "                for feature in features_list\n",
    "            ]\n",
    "            vectors_cqtnet_v1 = [\n",
    "                self.get_feature_vector(feature, \"cqtnet_v1\", **pre_process_options)\n",
    "                for feature in features_list\n",
    "            ]\n",
    "            return [(v1 + v2) / 2 for v1, v2 in zip(vectors_cqtnet, vectors_cqtnet_v1)]\n",
    "        \n",
    "        elif feature_name in [\"cqtnet\", \"coverhunter\"]:\n",
    "            return [\n",
    "                self.get_feature_vector(feature, feature_name, **pre_process_options)\n",
    "                for feature in features_list\n",
    "            ]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _ret_confidence(self, confidence, th=0.6):\n",
    "        conf = copy.deepcopy(confidence)\n",
    "        conf[confidence > th] = 1.0\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "fdbl = FeaturesDbLoader(\n",
    "    data_loader=dl,\n",
    "    cqtnet_repository=cqtnet_repository,\n",
    "    cqtnetv1_repository=cqtnetv1_repository,\n",
    "    coverhunter_repository=coverhunter_repository,\n",
    "    fusion_repository=fusion_repository,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit 1\n",
    "# features = fdbl.load_features(\"cqtnet\", \"coverhunter\", \"fusion\")\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pre_processed = fdbl.load_features(\"cqtnet\", \"coverhunter\", \"fusion\", granularity=\"all\", with_confidency=True)\n",
    "print(features_pre_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features in its collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdbl.add_features(\"cqtnet\", \"coverhunter\", \"fusion\", granularity=\"all\", with_confidency=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqtnet_repository.delete_collection()\n",
    "coverhunter_repository.delete_collection()\n",
    "cqtnetv1_repository.delete_collection()\n",
    "fusion_repository.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
